{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c27c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caaee8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_STR = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "IMSTART = \"<|im_start|>\"\n",
    "IMEND = \"<|im_end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "332b707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRAS = tuple((f\"<|extra_{i}|>\" for i in range(205)))\n",
    "SPECIAL_TOKENS = (\n",
    "    ENDOFTEXT,\n",
    "    IMSTART,\n",
    "    IMEND,\n",
    ") + EXTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67e2375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(tiktoken_tokenizer_path: str):\n",
    "    mergeable_ranks = load_tiktoken_bpe(tiktoken_tokenizer_path)\n",
    "    special_tokens = {\n",
    "        token: index\n",
    "        for index, token in enumerate(\n",
    "            SPECIAL_TOKENS, start=len(mergeable_ranks))\n",
    "    }\n",
    "    encoder = tiktoken.Encoding(\n",
    "        \"qwen\",\n",
    "        pat_str=PAT_STR,\n",
    "        mergeable_ranks=mergeable_ranks,\n",
    "        special_tokens=special_tokens,\n",
    "    )\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be19ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8bfee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_bytes_to_string(b):\n",
    "    byte_encoder = bytes_to_unicode()\n",
    "    return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdd00be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(mergeable_ranks: dict, token: bytes, max_rank: Optional[int] = None) -> list:\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4db3f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_and_merges(encoder):\n",
    "    mergeable_ranks = encoder._mergeable_ranks\n",
    "\n",
    "    merges = []\n",
    "    vocab = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        vocab[token_bytes_to_string(token)] = rank\n",
    "\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        merged = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(merged) == 2\n",
    "\n",
    "        merges.append(' '.join(map(token_bytes_to_string, merged)))\n",
    "\n",
    "    # also add special tokens\n",
    "    vocab.update(encoder._special_tokens)\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ba4fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_tiktoken(tiktoken_tokenizer_path: str, model_name, output_dir=None):\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = model_name\n",
    "        \n",
    "    encoder = get_encoder(tiktoken_tokenizer_path)\n",
    "\n",
    "    vocab, merges = generate_vocab_and_merges(encoder)\n",
    "\n",
    "    added_tokens = [{\n",
    "            \"id\": id,\n",
    "            \"content\": content,\n",
    "            \"single_word\": False,\n",
    "            \"lstrip\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"special\": True}\n",
    "        for content, id in encoder._special_tokens.items()\n",
    "    ]\n",
    "    \n",
    "    tokenizer_template = {\n",
    "        \"version\": \"1.0\",\n",
    "        \"truncation\": None,\n",
    "        \"padding\": None,\n",
    "        \"added_tokens\": added_tokens,\n",
    "        \"normalizer\": None,\n",
    "        \"pre_tokenizer\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": False,\n",
    "          \"trim_offsets\": True,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"post_processor\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": True,\n",
    "          \"trim_offsets\": False,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"decoder\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": True,\n",
    "          \"trim_offsets\": True,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"model\": {\n",
    "          \"type\": \"BPE\",\n",
    "          \"dropout\": None,\n",
    "          \"unk_token\": None,\n",
    "          \"continuing_subword_prefix\": \"\",\n",
    "          \"end_of_word_suffix\": \"\",\n",
    "          \"fuse_unk\": False,\n",
    "          \"byte_fallback\": False,\n",
    "          \"vocab\": vocab,\n",
    "          \"merges\": merges\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    tokenizer_config_template = {\n",
    "        \"model_max_length\": 8192,\n",
    "        \"tokenizer_class\": \"AutoTokenizer\"\n",
    "    }\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save to files\n",
    "    with open(os.path.join(output_dir, 'vocab.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(vocab, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'tokenizer.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(tokenizer_template, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'tokenizer_config.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(tokenizer_config_template, fp, indent=2, ensure_ascii=False)\n",
    "        \n",
    "output_dir = './tokenizer'\n",
    "tiktoken_tokenizer_path = \"./qwen_tokenizer/qwen.tiktoken\"\n",
    "convert_tiktoken(tiktoken_tokenizer_path, \"qwen\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "757a21ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:736\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m         )\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:736\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m         )\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "    \u001b[0;31m[... skipping similar frames: AutoTokenizer.from_pretrained at line 736 (2961 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:736\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m         )\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:686\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    688\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:537\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 537\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m \u001b[43mextract_commit_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_config_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m    540\u001b[0m     result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(reader)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/utils/hub.py:227\u001b[0m, in \u001b[0;36mextract_commit_hash\u001b[0;34m(resolved_file, commit_hash)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m commit_hash\n\u001b[0;32m--> 227\u001b[0m resolved_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m    228\u001b[0m search \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnapshots/([^/]+)/\u001b[39m\u001b[38;5;124m\"\u001b[39m, resolved_file)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m search \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/pathlib.py:1042\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m-> 1042\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/pathlib.py:683\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args, init)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 683\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/pathlib.py:676\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a str object or an os.PathLike \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject returning str, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(a))\n\u001b[0;32m--> 676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/pathlib.py:69\u001b[0m, in \u001b[0;36m_Flavour.parse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     67\u001b[0m altsep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltsep\n\u001b[1;32m     68\u001b[0m drv \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m part:\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d009b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
