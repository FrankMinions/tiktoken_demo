{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd95c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "FIM_PREFIX = \"<|fim_prefix|>\"\n",
    "FIM_MIDDLE = \"<|fim_middle|>\"\n",
    "FIM_SUFFIX = \"<|fim_suffix|>\"\n",
    "ENDOFPROMPT = \"<|endofprompt|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    ENDOFTEXT: 100257,\n",
    "    FIM_PREFIX: 100258,\n",
    "    FIM_MIDDLE: 100259,\n",
    "    FIM_SUFFIX: 100260,\n",
    "    ENDOFPROMPT: 100276,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77662663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(tiktoken_tokenizer_path: str):\n",
    "    mergeable_ranks = load_tiktoken_bpe(tiktoken_tokenizer_path)\n",
    "    encoder = tiktoken.Encoding(\n",
    "        \"cl100k_base\",\n",
    "        pat_str=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n",
    "        mergeable_ranks=mergeable_ranks,\n",
    "        special_tokens=special_tokens,\n",
    "    )\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9186ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_bytes_to_string(b):\n",
    "    byte_encoder = bytes_to_unicode()\n",
    "    return ''.join([byte_encoder[ord(char)] for char in b.decode('latin-1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(mergeable_ranks: dict, token: bytes, max_rank: Optional[int] = None) -> list:\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_and_merges(encoder):\n",
    "    mergeable_ranks = encoder._mergeable_ranks\n",
    "\n",
    "    merges = []\n",
    "    vocab = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        vocab[token_bytes_to_string(token)] = rank\n",
    "\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        merged = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(merged) == 2\n",
    "\n",
    "        merges.append(' '.join(map(token_bytes_to_string, merged)))\n",
    "\n",
    "    # Also add special tokens\n",
    "    vocab.update(encoder._special_tokens)\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "MODEL_INFO = {\n",
    "    'gpt-4': {\n",
    "        'tokenizer_class': 'GPT4Tokenizer',\n",
    "        'model_max_length': 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "def convert_tiktoken(tiktoken_tokenizer_path: str, model_name, output_dir=None):\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = model_name\n",
    "        \n",
    "    encoder = get_encoder(tiktoken_tokenizer_path)\n",
    "\n",
    "    vocab, merges = generate_vocab_and_merges(encoder)\n",
    "\n",
    "    added_tokens = [\n",
    "        {\n",
    "            \"id\": id,\n",
    "            \"content\": content,\n",
    "            \"single_word\": False,\n",
    "            \"lstrip\": False,\n",
    "            \"rstrip\": False,\n",
    "            \"normalized\": False,\n",
    "            \"special\": True\n",
    "        }\n",
    "        for content, id in encoder._special_tokens.items()\n",
    "    ]\n",
    "    \n",
    "    tokenizer_template = {\n",
    "        \"version\": \"1.0\",\n",
    "        \"truncation\": None,\n",
    "        \"padding\": None,\n",
    "        \"added_tokens\": added_tokens,\n",
    "        \"normalizer\": None,\n",
    "        \"pre_tokenizer\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": False,\n",
    "          \"trim_offsets\": True,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"post_processor\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": True,\n",
    "          \"trim_offsets\": False,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"decoder\": {\n",
    "          \"type\": \"ByteLevel\",\n",
    "          \"add_prefix_space\": True,\n",
    "          \"trim_offsets\": True,\n",
    "          \"use_regex\": True\n",
    "        },\n",
    "        \"model\": {\n",
    "          \"type\": \"BPE\",\n",
    "          \"dropout\": None,\n",
    "          \"unk_token\": None,\n",
    "          \"continuing_subword_prefix\": \"\",\n",
    "          \"end_of_word_suffix\": \"\",\n",
    "          \"fuse_unk\": False,\n",
    "          \"byte_fallback\": False,\n",
    "          \"vocab\": vocab,\n",
    "          \"merges\": merges\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    tokenizer_config_template = {\n",
    "        \"add_prefix_space\": False,\n",
    "        \"bos_token\": \"<|endoftext|>\",\n",
    "        \"clean_up_tokenization_spaces\": True,\n",
    "        \"eos_token\": \"<|endoftext|>\",\n",
    "        \"unk_token\": \"<|endoftext|>\"\n",
    "    }\n",
    "    \n",
    "    # Adds `model_max_length` and `tokenizer_class`\n",
    "    tokenizer_config_template.update(MODEL_INFO[model_name]) \n",
    "    \n",
    "    tokenizer_config_template = dict(sorted(tokenizer_config_template.items(), key=lambda x: x[0]))\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save to files\n",
    "    with open(os.path.join(output_dir, 'vocab.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(vocab, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'tokenizer.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(tokenizer_template, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'tokenizer_config.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump(tokenizer_config_template, fp, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    with open(os.path.join(output_dir, 'special_tokens_map.json'), 'w', encoding='utf-8') as fp:\n",
    "        json.dump({\n",
    "            \"bos_token\": \"<|endoftext|>\",\n",
    "            \"eos_token\": \"<|endoftext|>\",\n",
    "            \"unk_token\": \"<|endoftext|>\"\n",
    "        }, fp, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'merges.txt'), 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(merges))\n",
    "        \n",
    "output_dir = './openai_hf_tokenizer'\n",
    "tiktoken_tokenizer_path = \"./openai_tiktoken_tokenizer/cl100k_base.tiktoken\"\n",
    "convert_tiktoken(tiktoken_tokenizer_path, \"gpt-4\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('./openai_hf_tokenizer')\n",
    "tokenizer.decode(tokenizer.encode(\"Hello World!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
