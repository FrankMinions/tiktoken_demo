{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import collections\n",
    "import logging\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "from tqdm.contrib.logging import tqdm_logging_redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_STR = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, format=\"[%(asctime)s] %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10240060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tiktoken_bpe(tiktoken_bpe_file: str) -> \"dict[bytes, int]\":\n",
    "    contents = open(tiktoken_bpe_file, \"rb\").read()\n",
    "    return {\n",
    "        base64.b64decode(token): int(rank)\n",
    "        for token, rank in (line.split() for line in contents.splitlines() if line)\n",
    "    }\n",
    "\n",
    "\n",
    "def dump_tiktoken_bpe(bpe_ranks: \"dict[bytes, int]\", tiktoken_bpe_file: str) -> None:\n",
    "    with open(tiktoken_bpe_file, \"wb\") as f:\n",
    "        for token, rank in sorted(bpe_ranks.items(), key=lambda x: x[1]):\n",
    "            f.write(base64.b64encode(token) + b\" \" + str(rank).encode() + b\"\\n\")\n",
    "\n",
    "\n",
    "def bytes_to_pieces(the_bytes: bytes) -> \"tuple[bytes]\":\n",
    "    return tuple(bytes([byte]) for byte in the_bytes)\n",
    "\n",
    "\n",
    "def get_pairs(pieces: \"tuple[bytes]\") -> \"set[tuple[bytes, bytes]]\":\n",
    "    return set(zip(pieces[:-1], pieces[1:]))\n",
    "\n",
    "\n",
    "def get_stats(\n",
    "    vocab: \"dict[tuple[bytes, ...], int]\",\n",
    ") -> \"dict[tuple[bytes, bytes], int]\":\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs[(word[i], word[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(\n",
    "    pair: \"tuple[bytes, bytes]\", vocab: \"dict[tuple[bytes, ...], int]\"\n",
    ") -> \"dict[tuple[bytes, ...], int]\":\n",
    "    return {apply_bp(pieces, pair): freq for pieces, freq in vocab.items()}\n",
    "\n",
    "\n",
    "def apply_bp(\n",
    "    pieces: \"tuple[bytes, ...]\", pair: \"tuple[bytes, bytes]\"\n",
    ") -> \"tuple[bytes, ...]\":\n",
    "    new_pieces = []\n",
    "    first, second = pair\n",
    "    i = 0\n",
    "    while i < len(pieces):\n",
    "        try:\n",
    "            j = pieces.index(first, i)\n",
    "            new_pieces.extend(pieces[i:j])\n",
    "            i = j\n",
    "        except:\n",
    "            new_pieces.extend(pieces[i:])\n",
    "            break\n",
    "\n",
    "        if pieces[i] == first and i < len(pieces) - 1 and pieces[i + 1] == second:\n",
    "            new_pieces.append(first + second)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_pieces.append(pieces[i])\n",
    "            i += 1\n",
    "\n",
    "    return tuple(new_pieces)\n",
    "\n",
    "\n",
    "def bpe(word: bytes, merges: \"dict[bytes,int]\") -> \"tuple[bytes, ...]\":\n",
    "    pieces = bytes_to_pieces(word)\n",
    "    while len(pieces) > 1:\n",
    "        pairs = get_pairs(pieces)\n",
    "        pair = min(pairs, key=lambda pair: merges.get(pair[0] + pair[1], float(\"inf\")))\n",
    "\n",
    "        if pair[0] + pair[1] not in merges:\n",
    "            break\n",
    "        pieces = apply_bp(pieces, pair)\n",
    "        # logger.debug(f\"{[(p, p.decode('utf8', errors='replace')) for p in pieces]} {pair} {pieces}\")\n",
    "    return pieces\n",
    "\n",
    "\n",
    "def best_pair_sort_key(\n",
    "    item: \"tuple[dict[bytes, bytes], int]\",\n",
    ") -> \"tuple[int, int, int, str, bytes]\":\n",
    "    # prefer to use the highest frequency or shortest length or lexi sort, sligtly slower\n",
    "    pair, freq = item\n",
    "    pair_bytes = pair[0] + pair[1]\n",
    "    pair_byte_length = len(pair_bytes)\n",
    "    pair_str = pair_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    pair_str_length = len(pair_str)\n",
    "    return -freq, pair_str_length, pair_byte_length, pair_str, pair_bytes\n",
    "\n",
    "\n",
    "def learn_bpe(\n",
    "    freqs: \"dict[str,int]\", existing: \"dict[bytes, int]\"\n",
    ") -> \"tuple[bytes, bytes]\":\n",
    "    vocab = {bpe(k.encode(\"utf-8\"), existing): v for k, v in freqs.items()}\n",
    "    vocab = {key: value for key, value in vocab.items() if len(key) > 1}\n",
    "    new_merges = []\n",
    "    with tqdm_logging_redirect() as bar:\n",
    "        while vocab:\n",
    "            pairs = get_stats(vocab)\n",
    "\n",
    "            best, freq = min(pairs.items(), key=best_pair_sort_key)\n",
    "\n",
    "            logger.debug(\n",
    "                f'{best} ({(best[0]+best[1]).decode(\"utf-8\", errors=\"replace\")}) is selected as the next merge with freq {freq}'\n",
    "            )\n",
    "            new_merges.append(best)\n",
    "\n",
    "            vocab = merge_vocab(best, vocab)\n",
    "            vocab = {key: value for key, value in vocab.items() if len(key) > 1}\n",
    "            bar.update()\n",
    "\n",
    "    return new_merges\n",
    "\n",
    "\n",
    "def load_expand_vocab(path: Path) -> \"dict[str, int]\":\n",
    "    freqs = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\") as fin:\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            seg = line.strip().split(\"\\t\")\n",
    "            word, freq = \"\".join(seg[:-1]), seg[-1]\n",
    "            word = unicodedata.normalize(\"NFC\", word)\n",
    "            parts = re.findall(PAT_STR, word)\n",
    "            if len(parts) > 1:\n",
    "                logger.warning(\n",
    "                    f\"{word} would be pre-tokenized to {parts}, and thus cannot be added to vocabulary\"\n",
    "                )\n",
    "                continue\n",
    "            try:\n",
    "                freq = int(freq)\n",
    "            except ValueError as _:\n",
    "                freq = 1\n",
    "            if word in freqs:\n",
    "                logger.warning(\n",
    "                    f\"{word} is repeated, the frequency is increased by this much\"\n",
    "                )\n",
    "                freqs[word] += freq\n",
    "            else:\n",
    "                freqs[word] = freq\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3889db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_merges_by_bpe(\n",
    "    input_path: Path, output_path: Path, expand_path: Path, start_id: int\n",
    ") -> None:\n",
    "    mergeable_ranks = load_tiktoken_bpe(input_path)\n",
    "\n",
    "    if not start_id or start_id == -1:\n",
    "        start_id = len(mergeable_ranks)\n",
    "    elif start_id < len(mergeable_ranks):\n",
    "        logger.warning(\n",
    "            f\"start_id {start_id} is too small, existing merges will be overridden, DONOT DO THIS. changed to {len(mergeable_ranks)}\"\n",
    "        )\n",
    "        start_id = len(mergeable_ranks)\n",
    "    else:\n",
    "        start_id = start_id\n",
    "\n",
    "    expand_vocab_freqs = load_expand_vocab(expand_path)\n",
    "    for word in list(expand_vocab_freqs):\n",
    "        token = word.encode(\"utf-8\")\n",
    "        if token in mergeable_ranks:\n",
    "            logger.warning(f\"word {word} is already a token {token}, skipping\")\n",
    "            del expand_vocab_freqs[word]\n",
    "\n",
    "    logger.info(f\"number of existing merges: {len(mergeable_ranks)}\")\n",
    "    logger.info(f\"number of words for expanding: {len(expand_vocab_freqs)}\")\n",
    "\n",
    "    new_merges = learn_bpe(expand_vocab_freqs, mergeable_ranks)\n",
    "    logger.info(f\"number of newly learned merges: {len(new_merges)}\")\n",
    "\n",
    "    extra_merges = {p[0] + p[1]: i for i, p in enumerate(new_merges, start=start_id)}\n",
    "\n",
    "    dump_tiktoken_bpe(extra_merges, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"--input_path\", type=str, help=\"Path for input tiktoken file\")\n",
    "    parser.add_argument(\n",
    "        \"--output_path\",\n",
    "        type=str,\n",
    "        help=\"Path for output tiktoken file, containing only the new merges\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vocab_path\",\n",
    "        type=str,\n",
    "        help=\"Path for words needed adding, each line is a word and its frequency separated by \\\\t\",\n",
    "    )\n",
    "    # if the extended vocabulary is for fine-tuning, you better set those correctly (the default is for qwen.tiktoken)\n",
    "    # if the extended vocabulary is for pretraining from the start, no need\n",
    "    parser.add_argument(\n",
    "        \"--start_id\",\n",
    "        type=int,\n",
    "        default=151851,\n",
    "        help=\"The start id for new merges. For Qwen tokenizer, this should be 151851 (skipping the existing special tokens)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    make_new_merges_by_bpe(\n",
    "        args.input_path, args.output_path, args.vocab_path, args.start_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ae121",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
